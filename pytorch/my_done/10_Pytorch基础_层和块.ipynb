{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 层和块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0170, -0.1487, -0.2131, -0.5660, -0.3181, -0.3801, -0.1051,  0.0324,\n",
       "         -0.0014, -0.0204],\n",
       "        [-0.0153, -0.0353, -0.2139, -0.4092, -0.3671, -0.2834, -0.1464,  0.0227,\n",
       "          0.0082,  0.0724]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 多层感知机\n",
    "\"\"\" nn.Sequential 定义了一个特殊的 Module\"\"\"\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(20, 256), \n",
    "    nn.ReLU(), \n",
    "    nn.Linear(256, 10)\n",
    "    )\n",
    "\n",
    "X = torch.rand(size=(2,20), requires_grad=True)\n",
    "net(X)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "任何层以及神经网络都是 Module 的子类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1856,  0.0043,  0.2111, -0.1339, -0.1064, -0.1577, -0.0451, -0.0790,\n",
       "          0.0848,  0.2023],\n",
       "        [ 0.2732, -0.0770,  0.1067,  0.0679, -0.1388, -0.2292,  0.0382, -0.1848,\n",
       "          0.0928,  0.1425]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(20, 256)\n",
    "        self.out = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.out(F.relu(self.hidden(X)))\n",
    "\n",
    "net = MLP()\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1889,  0.0636, -0.0640,  0.1260, -0.2819, -0.1551,  0.0773,  0.0797,\n",
       "          0.0096, -0.0399],\n",
       "        [-0.1198,  0.1782,  0.0275,  0.1361, -0.1305, -0.0812,  0.0048,  0.0488,\n",
       "         -0.0102,  0.0547]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" 自定义Sequential \"\"\"\n",
    "class My_Sequential(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "        for block in args:\n",
    "            self._modules[block] = block    # block 有序字典\n",
    "    \n",
    "    def forward(self, X):\n",
    "        for block in self._modules.values():\n",
    "            X = block(X)\n",
    "        return X\n",
    "\n",
    "net = My_Sequential(\n",
    "    nn.Linear(20, 256), \n",
    "    nn.ReLU(), \n",
    "    nn.Linear(256, 10)\n",
    "    )\n",
    "\n",
    "X = torch.rand(size=(2,20), requires_grad=True)\n",
    "net(X)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 正向传播中自定义计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class FixedHiddenMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.rand_weight = torch.rand((20, 20), requires_grad=False)    # 随便放的值，不参与学习\n",
    "\n",
    "    def forward(self, X):\n",
    "        return sum(X) # 稀奇古怪的操作\n",
    "\n",
    "net = FixedHiddenMLP()\n",
    "net([1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1778, -0.1484, -0.0290, -0.1229,  0.2215,  0.0035, -0.1987, -0.1153,\n",
       "          0.2252,  0.0297, -0.1748, -0.1443, -0.1022, -0.1991, -0.3512, -0.1368,\n",
       "          0.0104,  0.1535, -0.1914, -0.1309],\n",
       "        [-0.1632, -0.1407,  0.0022, -0.1309,  0.2488,  0.0107, -0.1924, -0.0948,\n",
       "          0.1747,  0.0008, -0.1732, -0.1503, -0.1040, -0.2507, -0.3355, -0.1547,\n",
       "          0.0335,  0.1668, -0.1651, -0.1523]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" 混合搭配 \"\"\"\n",
    "class NestMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(20, 64), nn.ReLU(),\n",
    "                                 nn.Linear(64, 32), nn.ReLU())\n",
    "        self.linear = nn.Linear(32, 16)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.linear(self.net(X))\n",
    "\n",
    "chimera = nn.Sequential(NestMLP(), nn.Linear(16, 20))\n",
    "chimera(X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e68e5b16d81fa9059e1158f6a4d703d6dd8750a9ccfa1e75dbe04c34949a9a2e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
