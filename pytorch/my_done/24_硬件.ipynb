{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 硬件\n",
    "\n",
    "```python\n",
    "for i in range(len(a)):\n",
    "    c[i] = b[i] + a[i]\n",
    "\n",
    "c = a + b\n",
    "```\n",
    "\n",
    "后者比前者更优：\n",
    "- 前者需要调用n次函数，每次均有开销\n",
    "- 后者更容易被并行执行\n",
    "\n",
    "```c++\n",
    "#pragma omp for\n",
    "for (int i = 0; i < a.size(); i++) \n",
    "    c[i] = a[i] + b[i]\n",
    "```\n",
    "\n",
    "- TPU:使用脉动矩阵加速运算\n",
    "- 多GPU:模型并行&数据并行"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QA\n",
    "\n",
    "- 增加数据是提高泛化性的最优手段，高质量的数据\n",
    "- 框架会要求显示表示数据在GPU还是CPU\n",
    "- 复现论文：注重每一句话，但是只有20%才能实现\n",
    "    - 看看别人实现的代码，通常与论文不同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib_inline import backend_inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = 0.01\n",
    "# kernel_size=3x3 in=1 out=20\n",
    "W1 = torch.randn(size=(20, 1, 3, 3)) * scale\n",
    "b1 = torch.zeros(20)\n",
    "W2 = torch.randn(size=(50, 20, 5, 5)) * scale\n",
    "b2 = torch.zeros(50)\n",
    "W3 = torch.randn(size=(800, 128)) * scale\n",
    "b3 = torch.zeros(128)\n",
    "W4 = torch.randn(size=(128, 10)) * scale\n",
    "b4 = torch.zeros(10)\n",
    "param = [W1, b1, W2, b2, W3, b3, W4, b4]\n",
    "\n",
    "def lenet(X, params):\n",
    "    h1_conv = F.conv2d(input=X, weight=params[0], bias=params[1])\n",
    "    h1_activation = F.relu(h1_conv)\n",
    "    h1 = F.avg_pool2d(input=h1_activation, kernel_size=(2, 2), stride=2)\n",
    "    h2_conv = F.conv2d(input=h1, weight=params[2], bias=params[3])\n",
    "    h2_activation = F.relu(h2_conv)\n",
    "    h2 = F.avg_pool2d(input=h2_activation, kernel_size=(2, 2), stride=2)\n",
    "\n",
    "    h2 = h2.reshape([h2.shape[0], -1])\n",
    "    h3 = F.relu(torch.mm(h2, params[4]) + params[5])\n",
    "    y_hat = torch.mm(h3, params[6]) + params[7]\n",
    "    return y_hat\n",
    "\n",
    "loss = nn.CrossEntropyLoss(reduction=\"none\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b1 weight: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "b1 grad: None\n"
     ]
    }
   ],
   "source": [
    "def get_params(params, device):\n",
    "    new_params = [p.clone().to(device) for p in params]\n",
    "    for p in new_params:\n",
    "        p.requires_grad_()\n",
    "    return new_params\n",
    "\n",
    "def try_gpu(i=0):\n",
    "    if torch.cuda.device_count() >= i+1:\n",
    "        return torch.device(f\"cuda:{i}\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "new_params = get_params(param, try_gpu(0))\n",
    "print(\"b1 weight:\", new_params[1])\n",
    "print(\"b1 grad:\", new_params[1].grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[1., 1.]]), tensor([[2., 2.]], device='cuda:0')]\n",
      "[tensor([[3., 3.]]), tensor([[3., 3.]], device='cuda:0')]\n"
     ]
    }
   ],
   "source": [
    "def allreduce(data):\n",
    "    for i in range(1, len(data)):\n",
    "        data[0][:] += data[i].to(data[0].device) # [:] 原地操作\n",
    "    for i in range(1, len(data)):\n",
    "        data[i] = data[0].to(data[i].device)\n",
    "\n",
    "# data = [torch.ones((1, 2), dtype=torch.float32, device=try_gpu(i)) * (i + 1) for i in range(2)]\n",
    "device = [torch.device('cpu'), try_gpu()]\n",
    "data = [torch.ones((1, 2), dtype=torch.float32, device=d) * (i + 1) for i, d in enumerate(device)]\n",
    "print(data)\n",
    "allreduce(data)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[0, 1, 2, 3, 4],\n",
      "        [5, 6, 7, 8, 9]]), tensor([[10, 11, 12, 13, 14],\n",
      "        [15, 16, 17, 18, 19]], device='cuda:0')]\n"
     ]
    }
   ],
   "source": [
    "# 批量切开\n",
    "data = torch.arange(20).reshape(4, 5)\n",
    "# scatter 不能有cpu\n",
    "# split = torch.nn.parallel.scatter(data, device)\n",
    "def scatter(data, device):\n",
    "    num_device = len(device)\n",
    "    assert data.shape[0] % num_device == 0\n",
    "    length = data.shape[0] // num_device\n",
    "    y = []\n",
    "    for i in range(len(device)):\n",
    "        y.append(data[i*length:(i+1)*length, :].to(device[i]))\n",
    "    return y\n",
    "split = scatter(data, device)\n",
    "print(split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_batch(X, y, devices):\n",
    "    assert X.shape[0] == y.shape[0]\n",
    "    return (scatter(X, devices), scatter(y, devices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, y, device_params, devices, lr):\n",
    "    X_shards, y_shards = split_batch(X, y, devices)\n",
    "    ls = [\n",
    "        loss(lenet(lenet(X_shard, device_w), y_shard)).sum \n",
    "        for X_shard, y_shard, device_w in zip(X_shards, y_shards, device_params)\n",
    "    ]\n",
    "    for l in ls:\n",
    "        l.backward()\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(device_params[0])):\n",
    "            # 对每一层的参数对每一个device更新\n",
    "            allreduce(\n",
    "                [device_params[d][i] for d in range(len(device))]\n",
    "            )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 简洁实现\n",
    "```python\n",
    "net = nn.DataParallel(net, device_ids = devices)\n",
    "```\n",
    "\n",
    "## 目前应当使用DDP而不是DP!!\n",
    "\n",
    "- 理论上，num_gpu * n, batch_size * n, lr * n\n",
    "- 分布式需要考虑带宽\n",
    "- batch_size=1时，理论精度最佳\n",
    "- batch_size过大时，需要考虑lr的优化算法\n",
    "- 一般来说，$\\text{batch-size}\\sim 10\\times\\text{num-classes}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4654e147d6fe676f31a9f86e2485eea716359f8709963986145f7c2d0088ba8c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
